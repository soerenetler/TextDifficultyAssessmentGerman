{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/soeren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pyphen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_character_per_word(preprocessed_text):\n",
    "    characters_per_wors = []\n",
    "    for sentence in preprocessed_text:\n",
    "        characters_per_wors += [len(word[0]) for word in sentence if word[1]!=\"SYM\"]\n",
    "    return np.mean(characters_per_wors)\n",
    "\n",
    "test1 = [[[\"Ich\", \"\"], [\"studiere\", \"\"], [\"in\", \"\"], [\"Potsdam\", \"\"], [\".\", \"SYM\"]]]\n",
    "assert average_character_per_word(test1) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length(preprocessed_text):\n",
    "    '''given a preprocessed text (in RFTagger output format) the text length in token is returned'''\n",
    "    count = 0\n",
    "    for sentence in preprocessed_text:\n",
    "        count+= len(sentence)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(preprocessed_text):\n",
    "    ''' Average length of a sentence. All characters and symbols are included.'''\n",
    "    return np.mean([len(sentence) for sentence in preprocessed_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_syllable_word(preprocessed_text):\n",
    "    '''Average length of a sentence. SYM are excluded and not counted'''\n",
    "    pyphen.language_fallback('de_DE_variant1')\n",
    "    dic = pyphen.Pyphen(lang='de_DE')\n",
    "    counts = []\n",
    "    for sentence in preprocessed_text:\n",
    "        for word in sentence:\n",
    "            if word[1]!=\"SYM\":\n",
    "                counts.append(syllable_count(word[0], dic))\n",
    "    \n",
    "    return np.mean(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(string, dic):\n",
    "    '''The number of syllables of a string is returned'''\n",
    "    return len(dic.inserted(string).split(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flesch_Reading_Ease_Score(preprocessed_text):\n",
    "    ''' On a given text the score will produce a value between 1 and 100 where the higher\n",
    "    the value the easier the text would be. Documents scoring 30 are very difficult to\n",
    "    read while those scoring 70 should be easy to read.'''\n",
    "    asl = average_sentence_length(preprocessed_text)\n",
    "    asw = average_syllable_word(preprocessed_text)\n",
    "    \n",
    "    return 206.835 - (1.015 * asl) - (84.6 * asw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flesch_Kincaid_readability(preprocessed_text):\n",
    "    '''The Flesch-Kincaid readability formula (2.2) simplifies the Flesch score to produce a “grade\n",
    "    level” which is easily interpretable (i.e., a text with a grade level of eight according to the formula\n",
    "    could be thought appropriate for an eighth grader).'''\n",
    "    asl = average_sentence_length(preprocessed_text)\n",
    "    asw = average_syllable_word(preprocessed_text)\n",
    "    \n",
    "    return (0.4 * asl) + (12 * asw) - 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentag_hard_words(preprocessed_text):\n",
    "    pyphen.language_fallback('de_DE_variant1')\n",
    "    dic = pyphen.Pyphen(lang='de_DE')\n",
    "    count_hard_words=0\n",
    "    length=0\n",
    "    for sentence in preprocessed_text:\n",
    "        for word in sentence:\n",
    "            if word[1] != \"SYM\":\n",
    "                length+=1\n",
    "                if syllable_count(word[0], dic)>=3:\n",
    "                    count_hard_words+=1\n",
    "    if length == 0:\n",
    "        print(preprocessed_text)\n",
    "        return 0\n",
    "            \n",
    "    return count_hard_words/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog(preprocessed_text):\n",
    "    \"FOG = 0.4 * (percentag_hard_words + average_sentence_length)\"\n",
    "    hw = percentag_hard_words(preprocessed_text)\n",
    "    asl = average_sentence_length(preprocessed_text)\n",
    "                \n",
    "    return 0.4 * (hw + asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polysyllable_count(preprocessed_text):\n",
    "    '''the number of words with 3 or more syllables in 30 sentences\n",
    "    which shall be picked from the beginning, middle, and end of the document.'''\n",
    "    pyphen.language_fallback('de_DE_variant1')\n",
    "    dic = pyphen.Pyphen(lang='de_DE')\n",
    "    part_len = int(len(preprocessed_text)/3)\n",
    "    \n",
    "    if part_len > 10:\n",
    "        beginning_sentences = preprocessed_text[:part_len]\n",
    "        middle_sentences = preprocessed_text[part_len:-part_len]\n",
    "        end_sentences = preprocessed_text[-part_len:]\n",
    "        picked_sentences = random.choice(beginning_sentences, 10) + random.choice(middle_sentences, 10) + random.choice(end_sentences, 10)\n",
    "    else:\n",
    "        picked_sentences = preprocessed_text\n",
    "    \n",
    "    polysyllable_count = 0\n",
    "    for sentence in picked_sentences:\n",
    "        for word in sentence:\n",
    "            if syllable_count(word[0], dic)>=3:\n",
    "                polysyllable_count+=1\n",
    "    \n",
    "    return polysyllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smog(preprocessed_text):\n",
    "    '''3 + sqrt(normalized polysyllable count) \n",
    "    The polysyllable count is taken from 30 random sentences from beginning, middle and end of the text'''\n",
    "    psc = polysyllable_count(preprocessed_text)\n",
    "    return 3 + np.sqrt(psc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_3_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/30 [00:10<04:51, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_5_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [00:19<04:32,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_7_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [00:28<04:15,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_14_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [00:32<03:32,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_12_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 5/30 [00:36<03:04,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_10_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 6/30 [00:46<03:05,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/30 [00:50<02:46,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_9_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 8/30 [00:59<02:44,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_3_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 9/30 [01:03<02:28,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_7_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 10/30 [01:07<02:14,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_11_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 11/30 [01:10<02:02,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_13_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 12/30 [01:14<01:51,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_8_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 13/30 [01:22<01:48,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_4_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 14/30 [01:31<01:44,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_12_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 15/30 [01:40<01:40,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_13_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 16/30 [01:49<01:35,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_9_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 17/30 [01:53<01:26,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_8_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 18/30 [01:56<01:17,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_15_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 19/30 [02:05<01:12,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 20/30 [02:09<01:04,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_5_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 21/30 [02:13<00:57,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_10_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 22/30 [02:17<00:49,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_6_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 23/30 [02:20<00:42,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_15_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 24/30 [02:24<00:36,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_1_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 25/30 [02:34<00:30,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_6_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 26/30 [02:43<00:25,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_14_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 27/30 [02:51<00:19,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_11_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 28/30 [02:59<00:12,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 29/30 [03:09<00:06,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_4_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:12<00:00,  6.43s/it]\n"
     ]
    }
   ],
   "source": [
    "input_files = os.listdir(\"/home/soeren/Documents/TextDifficultyAssessmentGerman/datasets/01_RawDataset\")\n",
    "input_files = [input_file for input_file in input_files if not input_file in os.listdir(\"/home/soeren/Documents/TextDifficultyAssessmentGerman/datasets/02_BasicFeatures\")]\n",
    "\n",
    "def save_df(path,df):\n",
    "    df.to_csv(path,sep=\"|\",index=False)\n",
    "\n",
    "for input_file in tqdm(input_files):\n",
    "    df=pd.read_csv(\"datasets/01_RawDataset/\" + input_file, sep=\"|\", lineterminator='\\n')\n",
    "    print(input_file)\n",
    "    df[\"preprocessedText\"]  = [ast.literal_eval(text) for text in df[\"RFTagger\"]]\n",
    "    df[\"BF-average_sentence_length\"] = [average_sentence_length(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-average_syllable_word\"] = [average_syllable_word(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-Flesch_Reading_Ease_Score\"] = [Flesch_Reading_Ease_Score(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-Flesch_Kincaid_readability\"] = [Flesch_Kincaid_readability(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-percentag_hard_words\"] = [percentag_hard_words(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-fog\"] = [fog(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-polysyllable_count\"] = [polysyllable_count(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-smog\"] = [smog(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-average_character_per_word\"] = [average_character_per_word(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-text_length\"] = [text_length(text) for text in df[\"preprocessedText\"]]\n",
    "\n",
    "    filename = \"datasets/02_BasicFeatures/\" + input_file\n",
    "    save_df(filename,df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
