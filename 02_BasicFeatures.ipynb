{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/soeren/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pyphen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_character_per_word(preprocessed_text):\n",
    "    characters_per_wors = []\n",
    "    for sentence in preprocessed_text:\n",
    "        characters_per_wors += [len(word[0]) for word in sentence if word[1]!=\"SYM\"]\n",
    "    return np.mean(characters_per_wors)\n",
    "\n",
    "test1 = [[[\"Ich\", \"\"], [\"studiere\", \"\"], [\"in\", \"\"], [\"Potsdam\", \"\"], [\".\", \"SYM\"]]]\n",
    "assert average_character_per_word(test1) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length(preprocessed_text):\n",
    "    count = 0\n",
    "    for sentence in preprocessed_text:\n",
    "        count+= len(sentence)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(preprocessed_text):\n",
    "    ''' Average length of a sentence. All characters and symbols are included.'''\n",
    "    return np.mean([len(sentence) for sentence in preprocessed_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_syllable_word(preprocessed_text):\n",
    "    '''Average length of a sentence. SYM are excluded and not counted'''\n",
    "    pyphen.language_fallback('de_DE_variant1')\n",
    "    dic = pyphen.Pyphen(lang='de_DE')\n",
    "    counts = []\n",
    "    for sentence in preprocessed_text:\n",
    "        for word in sentence:\n",
    "            if word[1]!=\"SYM\":\n",
    "                counts.append(syllable_count(word[0], dic))\n",
    "    \n",
    "    return np.mean(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(string, dic):\n",
    "    '''The number of syllables of a string is returned'''\n",
    "    return len(dic.inserted(string).split(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flesch_Reading_Ease_Score(preprocessed_text):\n",
    "    ''' On a given text the score will produce a value between 1 and 100 where the higher\n",
    "    the value the easier the text would be. Documents scoring 30 are very difficult to\n",
    "    read while those scoring 70 should be easy to read.'''\n",
    "    asl = average_sentence_length(preprocessed_text)\n",
    "    asw = average_syllable_word(preprocessed_text)\n",
    "    \n",
    "    return 206.835 - (1.015 * asl) - (84.6 * asw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flesch_Kincaid_readability(preprocessed_text):\n",
    "    '''The Flesch-Kincaid readability formula (2.2) simplifies the Flesch score to produce a “grade\n",
    "    level” which is easily interpretable (i.e., a text with a grade level of eight according to the formula\n",
    "    could be thought appropriate for an eighth grader).'''\n",
    "    asl = average_sentence_length(preprocessed_text)\n",
    "    asw = average_syllable_word(preprocessed_text)\n",
    "    \n",
    "    return (0.4 * asl) + (12 * asw) - 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentag_hard_words(preprocessed_text):\n",
    "    pyphen.language_fallback('de_DE_variant1')\n",
    "    dic = pyphen.Pyphen(lang='de_DE')\n",
    "    count_hard_words=0\n",
    "    length=0\n",
    "    for sentence in preprocessed_text:\n",
    "        for word in sentence:\n",
    "            if word[1] != \"SYM\":\n",
    "                length+=1\n",
    "                if syllable_count(word[0], dic)>=3:\n",
    "                    count_hard_words+=1\n",
    "    if length == 0:\n",
    "        print(preprocessed_text)\n",
    "        return 0\n",
    "            \n",
    "    return count_hard_words/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog(preprocessed_text):\n",
    "    hw = percentag_hard_words(preprocessed_text)\n",
    "    asl = average_sentence_length(preprocessed_text)\n",
    "                \n",
    "    return 0.4 * (hw + asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polysyllable_count(preprocessed_text):\n",
    "    '''the number of words with 3 or more syllables in 30 sentences\n",
    "    which shall be picked from the beginning, middle, and end of the document.'''\n",
    "    pyphen.language_fallback('de_DE_variant1')\n",
    "    dic = pyphen.Pyphen(lang='de_DE')\n",
    "    part_len = int(len(preprocessed_text)/3)\n",
    "    \n",
    "    if part_len > 10:\n",
    "        beginning_sentences = preprocessed_text[:part_len]\n",
    "        middle_sentences = preprocessed_text[part_len:-part_len]\n",
    "        end_sentences = preprocessed_text[-part_len:]\n",
    "        picked_sentences = random.choice(beginning_sentences, 10) + random.choice(middle_sentences, 10) + random.choice(end_sentences, 10)\n",
    "    else:\n",
    "        picked_sentences = preprocessed_text\n",
    "    \n",
    "    polysyllable_count = 0\n",
    "    for sentence in picked_sentences:\n",
    "        for word in sentence:\n",
    "            if syllable_count(word[0], dic)>=3:\n",
    "                polysyllable_count+=1\n",
    "    \n",
    "    return polysyllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smog(preprocessed_text):\n",
    "    psc = polysyllable_count(preprocessed_text)\n",
    "    return 3 + np.sqrt(psc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_Preprocessing_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:38<00:00, 38.15s/it]\n"
     ]
    }
   ],
   "source": [
    "input_files = os.listdir(\"/home/soeren/Documents/TextDifficultyAssessmentGerman/datasets/RandomText\")\n",
    "input_files = [input_file for input_file in input_files if not input_file.startswith(\"FeatureSet\")]\n",
    "input_files = [\"01_Preprocessing_df.csv\"]\n",
    "\n",
    "def save_df(path,df):\n",
    "    df.to_csv(path,sep=\"|\",index=False)\n",
    "\n",
    "for input_file in tqdm(input_files):\n",
    "    df=pd.read_csv(\"datasets/\" + input_file, sep=\"|\", lineterminator='\\n')\n",
    "    print(input_file)\n",
    "    df[\"preprocessedText\"]  = [ast.literal_eval(text) for text in df[\"RFTagger\"]]\n",
    "    df[\"BF-average_sentence_length\"] = [average_sentence_length(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-average_syllable_word\"] = [average_syllable_word(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-Flesch_Reading_Ease_Score\"] = [Flesch_Reading_Ease_Score(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-Flesch_Kincaid_readability\"] = [Flesch_Kincaid_readability(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-percentag_hard_words\"] = [percentag_hard_words(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-fog\"] = [fog(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-polysyllable_count\"] = [polysyllable_count(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-smog\"] = [smog(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-average_character_per_word\"] = [average_character_per_word(text) for text in df[\"preprocessedText\"]]\n",
    "    df[\"BF-text_length\"] = [text_length(text) for text in df[\"preprocessedText\"]]\n",
    "\n",
    "    filename = \"datasets/RandomText/FeatureSet1_BF/\" + input_file\n",
    "    save_df(filename,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
