{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Tree Complexity and Specific Syntactic Constructions Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import Counter, defaultdict\n",
    "import de_core_news_sm\n",
    "import itertools\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/07_SyntaxDependecy_df.csv\", sep=\"|\", lineterminator = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg. Length of a Clause # W / # C - DONE\n",
    "#Avg. Sentence Length # W / # S - Sören has it\n",
    "#Avg. Length of a T-Unit # W / # TU\n",
    "#Avg. Num. Clauses per Sentence # C / # S - DONE\n",
    "#Avg. Num. T-Units per Sentence # TU / # S \n",
    "#Avg. Num. Clauses per T-Unit #C / # TU - per sentence\n",
    "#Avg. Num. Complex-T-Units per T-Unit # comp. TU / # TU\n",
    "#Avg. Num. Dep. Clause per Clause # DC / # C -DONE\n",
    "#Avg. Num. Dep. Clause per T-Unit # DC / # TU -DONE\n",
    "#Avg. Num. Co-ordinate Phrases per Clause # CP / # C -DONE\n",
    "#Avg. Num. Co-ordinate Phrases per T-Unit # CP / # TU -DON\n",
    "#Avg. Num. Complex Nominals per Clause # compl. Nom. / # C -DONE-\n",
    "#Avg. Num. Complex Nominals per T-Unit # compl. Nom. / # TU -DONE\n",
    "#Avg. Num. VPs per T-Unit # VP / # TU -DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def break_into_clauses(text):\n",
    "    #takes text as a string a input\n",
    "    #returns a list of clauses as lists\n",
    "    clauses = []\n",
    "    clause = []\n",
    "    for ind, element in enumerate(word_tokenize(text)):\n",
    "        if element == \"S\" and clause != []:\n",
    "            clauses.append(clause)\n",
    "            clause = []\n",
    "        else:\n",
    "            clause.append(element) \n",
    "                        \n",
    "    clauses.append(clause)   \n",
    "    return clauses[1:]\n",
    "\n",
    "def list_of_clause_lengths(text):\n",
    "    #takes a text as string as input\n",
    "    #returns a list of numbers - number of elements in a clause\n",
    "    clause_lengths = []\n",
    "    clause_length = 0\n",
    "    clauses = break_into_clauses(text)\n",
    "    for clause in clauses:\n",
    "        for ind, element in enumerate(clause):\n",
    "            if element[-1].islower(): \n",
    "                clause_length += 1 \n",
    "            \n",
    "        clause_lengths.append(clause_length)\n",
    "        clause_length = 0\n",
    "           \n",
    "    clause_lengths.append(clause_length)    \n",
    "            \n",
    "    return clause_lengths[:-1]     \n",
    "\n",
    "def av_pos_per_clause_or_sentence(text, pos, clause_or_sentence):\n",
    "    no_pos = len([word for word in word_tokenize(text) if word == pos])\n",
    "    no_sentences = len(ast.literal_eval(text))\n",
    "    no_clauses = len(break_into_clauses(text))\n",
    "   \n",
    "    if clause_or_sentence == \"clause\":\n",
    "        if no_sentences == 0: return 0\n",
    "        return no_pos / no_sentences\n",
    "    else:\n",
    "        if no_clauses == 0: return 0\n",
    "        return no_pos / no_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_clause_length(text):\n",
    "    clause_lenghts = list_of_clause_lengths(text)\n",
    "    return np.mean(clause_lenghts)\n",
    "\n",
    "def max_clause_length(text):\n",
    "    clause_lenghts = list_of_clause_lengths(text)\n",
    "    if len(clause_lenghts) ==0:\n",
    "        return 0\n",
    "    return max(clause_lenghts)\n",
    "\n",
    "def av_num_clauses_per_sentence(text):\n",
    "    lengths = []\n",
    "    sentences = ast.literal_eval(text)\n",
    "    for sentence in sentences:\n",
    "        lengths.append(len(break_into_clauses(sentence)))\n",
    "    return np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_pronouns = [\"KOUS\", \"PRELAT\" \"PRELS\"]\n",
    "def av_num_dep_clauses_per_sentence(text):\n",
    "    no_dep_clauses = len([word for word in word_tokenize(text) if word in dependency_pronouns])\n",
    "    no_sentences = len(ast.literal_eval(text))\n",
    "    \n",
    "    return no_dep_clauses / no_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_num_dep_clauses_per_clause(text):\n",
    "    no_dep_clauses = len([word for word in word_tokenize(text) if word in dependency_pronouns])\n",
    "    no_clauses = len(break_into_clauses(text))\n",
    "    \n",
    "    if no_clauses == 0:\n",
    "        return 0\n",
    "    \n",
    "    return no_dep_clauses / no_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KON is coordinating conjunction\n",
    "def av_coordinations_per_sentence(text):    \n",
    "    return av_pos_per_clause_or_sentence(text, \"KON\", \"sentence\")\n",
    "\n",
    "def av_coordinations_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"KON\", \"clause\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg. Num. NPs per Sentence #NP / # S -DONE\n",
    "#Avg. Num. VPs per Sentence # VP / # S -DONE\n",
    "#Avg. Num. PPs per Sentence # VZ / # S -DONE\n",
    "#Avg. Num. VZs per Sentence # PP / # S -DONE\n",
    "#Avg. Num. NPs per Clause # NP / # C -DONE\n",
    "#Avg. Num. VPs per Clause # VP / # C -DONE\n",
    "#Avg. Num. PPs per Clause # PP / # C -DONE\n",
    "#Avg. Num. VZs per Clause # VZ / # C -DONE\n",
    "\n",
    "#Avg. Length of a NP sum(len(NP)) / # NP -- i have this with dependencies\n",
    "#Avg. Length of a VP sum(len(VP)) / # NP -- i have this with dependencies\n",
    "#Avg. Length of a PP sum(len(PP)) / # NP -- i have this with dependencies\n",
    "#Avg. Num. Dep. Clauses per Sentence # DC / # S -- this is up\n",
    "#Avg. Num. Complex T-Units per Sentence #compl. TU/ # S\n",
    "#Avg. Num. Co-ordinate Phrases per Sentence # CP / # S -- this is up\n",
    "#Avg. Parse Tree Height sum(parseTreeHeight) / # S -- done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_num_nps_per_sentence(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"NP\", \"sentence\")\n",
    "\n",
    "def av_num_vps_per_sentence(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"VP\", \"sentence\")\n",
    "\n",
    "def av_num_vzs_per_sentence(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"VZ\", \"sentence\")\n",
    "\n",
    "def av_num_pps_per_sentence(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"PP\", \"sentence\")\n",
    "\n",
    "def av_num_nps_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"NP\", \"clause\")\n",
    "\n",
    "def av_num_vps_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"VP\", \"clause\")\n",
    "\n",
    "def av_num_vzs_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"VZ\", \"clause\")\n",
    "\n",
    "def av_num_pps_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"PP\", \"clause\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg. Num. Non-Terminals Per Sentence # NTs / # S - DONE\n",
    "#Avg. Num. Non-Terminal Per Words # NTs / # W - DONE\n",
    "#Avg. Num. Modifers Per NP # modifersInNPs / # NP -- dependency\n",
    "#Avg. Num. Modifers Per VP # modif ersInVPs / # VPs -- dependency\n",
    "#Passive Voice - Sentence Ratio # passiveVoice / # S -- done\n",
    "#Passive Voice - Clause Ratio # passiveVoice /# C -- done\n",
    "\n",
    "#Dep. Clauses with Conj. to dep. Clause Ratio # DC w. Conj. / # DC\n",
    "#Conjunctional Clauses Ratio # Conj. C / # dep. C w. Conj.\n",
    "#Interrogative Clauses Ratio # Inter. C / # dep. C w. Conj. -- i think so\n",
    "#Relative Clauses Ratio # Rel. C / # DC w. Conj.  -- i think so\n",
    "#Dep. Clauses w.o. Conj. to dep. Clause Ratio # DC w.o. Conj. / # DC\n",
    "#`satzwertige Infnitive' to Clause Ratio # satzInf / # DC -- i think so\n",
    "\n",
    "# + separated verbs\n",
    "# + sein/haben ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_non_terminals_per_word(text):\n",
    "    text = re.sub('[()]', '', text)\n",
    "\n",
    "    no_words = len([word for word in word_tokenize(text) if word[:-1].islower()])\n",
    "    no_non_terminals = len([word for word in word_tokenize(text) if word[:-1].isupper()]) - no_words\n",
    "    if no_words != 0:\n",
    "        return no_non_terminals / no_words\n",
    "\n",
    "def av_non_terminals_per_sentence(text):\n",
    "    text = re.sub('[()]', '', text)\n",
    "    \n",
    "    no_words = len([word for word in word_tokenize(text) if word[:-1].islower()])\n",
    "    no_sentences = len(ast.literal_eval(text))\n",
    "    no_non_terminals = len([word for word in word_tokenize(text) if word[:-1].isupper()]) - no_words\n",
    "    \n",
    "    if no_sentences != 0:\n",
    "        return no_non_terminals / no_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_passives(text):\n",
    "    clauses = break_into_clauses(text)\n",
    "    num = 0\n",
    "    for clause in clauses:\n",
    "        for ind, element in enumerate(clause):\n",
    "            if (element == \"VAFIN\" and (clause[ind+1] == \"wird\" or clause[ind+1] == \"werden\" or clause[ind+1] == \"wurden\") \n",
    "            and \"VVPP\" in clause):                       \n",
    "                num += 1\n",
    "                \n",
    "    return num\n",
    "\n",
    "def no_passives_per_sentence(text):\n",
    "    return no_passives(text) / len(ast.literal_eval(text))\n",
    "\n",
    "def no_passives_per_clause(text):\n",
    "    if len(break_into_clauses(text)) == 0: return 0\n",
    "    return no_passives(text) / len(break_into_clauses(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zu_infinitive_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"KOUI\", \"clause\")\n",
    "\n",
    "def zu_infinitive_per_sentence(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"KOUI\", \"sentence\")\n",
    "\n",
    "def separated_verb_per_clause(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"PTKVZ\", \"clause\")\n",
    "\n",
    "def separated_verb_per_sentence(text):\n",
    "    return av_pos_per_clause_or_sentence(text, \"PTKVZ\", \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this needs the cleanedText!\n",
    "nlp = de_core_news_sm.load()\n",
    "def sein_haben_ratio(text):\n",
    "    doc = nlp(text)\n",
    "    no = 0\n",
    "    for tok in doc:\n",
    "        if tok.lemma_ == \"sein\" or tok.lemma_ == \"haben\" or tok.lemma_ == \"habe\":\n",
    "            no += 1\n",
    "    return no/len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PWS substituting interrogative pronoun wer, was\n",
    "#PWAT attributive interrogative pronoun welche [Farbe], wessen [Hut]\n",
    "#PWAV adverbial interrogative or relativ\n",
    "interrogative_pronouns = [\"PWS\", \"PWAT\", \"PWAV\"]\n",
    "def av_inter_clause_per_sentence(text):\n",
    "    no_inter_clauses = len([word for word in word_tokenize(text) if word in interrogative_pronouns])\n",
    "    no_sentences = len(ast.literal_eval(text))\n",
    "    \n",
    "    return no_inter_clauses / no_sentences\n",
    "\n",
    "def av_inter_clause_per_clause(text):\n",
    "    no_inter_clauses = len([word for word in word_tokenize(text) if word in interrogative_pronouns])\n",
    "    no_clauses = len(break_into_clauses(text))\n",
    "    \n",
    "    if no_clauses == 0: return 0\n",
    "    return no_inter_clauses / no_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_clauses_to_dependent_clauses_ratio(text):\n",
    "    no_rel_clauses = len([word for word in text if word == \"PRELAT\" or word == \"PRELS\"])\n",
    "    no_dep_clauses = no_rel_clauses + len([word for word in text if word == \"KOUS\"])\n",
    "    \n",
    "    if no_rel_clauses != 0:\n",
    "        return no_rel_clauses / no_dep_clauses\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_feature_functions = [av_clause_length, max_clause_length, av_num_clauses_per_sentence, av_num_dep_clauses_per_sentence,\n",
    "                         av_num_dep_clauses_per_clause, av_coordinations_per_sentence, av_coordinations_per_clause, \n",
    "                         av_num_nps_per_sentence, av_num_vps_per_sentence, av_num_vzs_per_sentence,\n",
    "                         av_num_pps_per_sentence, av_num_nps_per_clause, av_num_vps_per_clause, av_num_vzs_per_clause,\n",
    "                         av_num_pps_per_clause, av_non_terminals_per_word, av_non_terminals_per_sentence, no_passives,\n",
    "                         no_passives_per_sentence, no_passives_per_clause, zu_infinitive_per_clause, zu_infinitive_per_sentence,\n",
    "                         separated_verb_per_clause, separated_verb_per_sentence, av_inter_clause_per_clause, \n",
    "                         av_inter_clause_per_sentence, relative_clauses_to_dependent_clauses_ratio]\n",
    "\n",
    "SC_feature_function_with_clearedText = sein_haben_ratio                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/soeren/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/soeren/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 1/27 [00:25<10:50, 25.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 2/27 [00:48<10:12, 24.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 3/27 [01:15<10:06, 25.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 4/27 [01:39<09:29, 24.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▊        | 5/27 [02:25<10:38, 29.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 6/27 [03:11<11:10, 31.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 7/27 [03:57<11:19, 33.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 8/27 [04:44<11:15, 35.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 9/27 [05:31<11:02, 36.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 10/27 [06:17<10:41, 37.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 11/27 [07:03<10:16, 38.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 12/27 [07:50<09:47, 39.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 13/27 [08:35<09:15, 39.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 14/27 [09:21<08:41, 40.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 15/27 [10:06<08:04, 40.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 16/27 [10:35<07:17, 39.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 17/27 [11:06<06:31, 39.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 18/27 [11:28<05:44, 38.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 19/27 [11:52<04:59, 37.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 20/27 [13:00<04:33, 39.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 21/27 [13:45<03:55, 39.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 22/27 [14:30<03:17, 39.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 23/27 [15:15<02:39, 39.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 24/27 [16:00<02:00, 40.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 25/27 [16:46<01:20, 40.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▋| 26/27 [17:08<00:39, 39.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 27/27 [17:09<00:00, 38.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "def save_df(path,df):\n",
    "    df.to_csv(path,sep=\"|\",index=False)\n",
    "    \n",
    "input_files = os.listdir(\"/home/soeren/Documents/TextDifficultyAssessmentGerman/datasets/RandomText/FeatureSet6_SD\")\n",
    "input_files = [\"01_Preprocessing_df.csv\"]\n",
    "\n",
    "for input_file in input_files:\n",
    "    df = pd.read_csv(\"datasets/RandomText/FeatureSet6_SD/\" + input_file, sep=\"|\", lineterminator = '\\n')\n",
    "    \n",
    "    for function in tqdm(SC_feature_functions):\n",
    "        df[\"SC-\"+function.__name__]  = [function(text) for text in df[\"parsedText\"]]\n",
    "\n",
    "    df[\"SC-sein_haben_ratio\"] = [sein_haben_ratio(text) for text in df[\"cleanedText\"]]\n",
    "\n",
    "    filename = \"datasets/RandomText/FeatureSet7_SC/\" + input_file\n",
    "    save_df(filename,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['(ROOT (S (PP (APPR Zu) (PPOSAT meiner) (NN F...\n",
       "1       ['(ROOT (S (NP (PPOSAT Mein) (NN Name)) (VAFIN...\n",
       "2       ['(ROOT (NUR (ITJ Hallo) ($. !)))', '(ROOT (S ...\n",
       "3       ['(ROOT (CS (S (NE Ricarda) (VAFIN ist) (AP (N...\n",
       "4       ['(ROOT (S (NP (NN Frau) (NE Meier)) (VVFIN ge...\n",
       "5       ['(ROOT (S (PPER Es) (VAFIN ist) (NP (CARD 7) ...\n",
       "6       ['(ROOT (S (ADJD Lieber) (NP (NE Thomas)) ($. ...\n",
       "7       ['(ROOT (S (NE Hartmut) (VAFIN hat) (VP (PP (A...\n",
       "8       ['(ROOT (S (NE Jan) (VAFIN hat) (NP (ART einen...\n",
       "9       ['(ROOT (S (ADV Heute) (VAFIN ist) (NP (ART de...\n",
       "10      ['(ROOT (S (MPN (NE Martin) (NE Stubbe)) (VAFI...\n",
       "11      ['(ROOT (CS (S (ISU (ITJ Hallo) (NE Marion))) ...\n",
       "12      ['(ROOT (S (ADV Heute) (VAFIN ist) (NN Freitag...\n",
       "13      ['(ROOT (S (PP (APPRART Im) (NN Sommer)) (VVFI...\n",
       "14      ['(ROOT (CS (S (NE Tina) (VAFIN ist) (AP (ADJD...\n",
       "15      ['(ROOT (S (PPER Wir) (VVFIN fahren) (PP (APPR...\n",
       "16      ['(ROOT (S (NP (NN Frau) (NE Meier)) (VVFIN ge...\n",
       "17      ['(ROOT (CS (S (ADV Sehr) (CNP (NP (ADJA geehr...\n",
       "18      ['(ROOT (S (NP (ART Der) (NN Müritzsee)) (VAFI...\n",
       "19      ['(ROOT (S (ADV Heute) (VAFIN ist) (NP (ART de...\n",
       "20      ['(ROOT (S (CNP (NE Anna) (KON und) (NE Stefan...\n",
       "21      ['(ROOT (NUR (NP (PIDAT Viele) (NN Leute)) (NP...\n",
       "22      ['(ROOT (S (NP (PPOSAT Mein) (NN Name)) (VAFIN...\n",
       "23      ['(ROOT (S (NE Tom) (VMFIN will) (VP (PP (APPR...\n",
       "24      [\"(ROOT (NUR (XY '')))\", '(ROOT (S (CNP (NN Ma...\n",
       "25      ['(ROOT (NUR (ITJ Hurra) ($. !)))', '(ROOT (S ...\n",
       "26      ['(ROOT (S (ADV Lange) (VAFIN habe) (PPER ich)...\n",
       "27      ['(ROOT (S (VMFIN Kannst) (PPER du) (VVINF err...\n",
       "28      [\"(ROOT (NUR (NN Arzt) ($. :) (XY '')))\", '(RO...\n",
       "29      [\"(ROOT (NUR (NE Lena) ($. :) (XY '')))\", '(RO...\n",
       "                              ...                        \n",
       "2535    ['(ROOT (S (NP (ART Der) (NN Einfluss) (NP (AR...\n",
       "2536    ['(ROOT (S (NN Politik) (VAFIN hat) (VP (PP (A...\n",
       "2537    ['(ROOT (NUR (S (NP (PPER Ich)) (VVFIN finde) ...\n",
       "2538    ['(ROOT (CS (S (NP (ART Der) (NN Einfluss) (NP...\n",
       "2539    ['(ROOT (S (NP (PPOSAT Meine) (ADJA bisherige)...\n",
       "2540    ['(ROOT (S (NP (ART Der) (NN Einfluss) (NP (AR...\n",
       "2541    ['(ROOT (S (NN Politik) (VAFIN hat) (VP (PP (A...\n",
       "2542    ['(ROOT (CS (S (PPER Es) (VAFIN ist) (AP (ADV ...\n",
       "2543    ['(ROOT (S (NN Politik) (VAFIN hat) (VP (PP (A...\n",
       "2544    ['(ROOT (S (PPER Ich) (VVFIN denke) (CS (S (KO...\n",
       "2545    ['(ROOT (NUR (S (NP (ART Der) (NN Einfluss) (N...\n",
       "2546    ['(ROOT (S (ADV Heute) (VMFIN müssen) (NN Juge...\n",
       "2547    ['(ROOT (NUR (S (NP (ART Der) (NN Einfluss) (N...\n",
       "2548    ['(ROOT (S (ADV Heute) (VMFIN müssen) (NN Juge...\n",
       "2549    ['(ROOT (NUR (S (NP (ART Der) (NN Einfluss) (N...\n",
       "2550    ['(ROOT (S (ADV Heute) (VMFIN müssen) (NN Juge...\n",
       "2551    ['(ROOT (S (NP (ART Der) (NN Einfluss) ($, ,) ...\n",
       "2552    ['(ROOT (S (NN Studiengebühren) (VVFIN diskrim...\n",
       "2553    ['(ROOT (NUR (S (NP (ART Der) (NN Einfluss) (N...\n",
       "2554    ['(ROOT (S (NN Politik) (VAFIN hat) (VP (PP (A...\n",
       "2555    ['(ROOT (S (NP (ART Der) (NN Einfluss) (NP (AR...\n",
       "2556    ['(ROOT (S (NN Studiengebühren) (VVFIN diskrim...\n",
       "2557    ['(ROOT (CS (S (PPER Es) (VVFIN gibt) (NP (PID...\n",
       "2558    ['(ROOT (NUR (S (NP (ART Der) (NN Einfluss) (N...\n",
       "2559    ['(ROOT (S (NN Politik) (VAFIN hat) (VP (PP (A...\n",
       "2560    ['(ROOT (S (NP (ART Der) (NN Einfluss) (NP (AR...\n",
       "2561    ['(ROOT (S (NN Politik) (VAFIN hat) (VP (PP (A...\n",
       "2562    ['(ROOT (NUR (S (NN Vokabeln) (VVFIN lernen)))...\n",
       "2563    ['(ROOT (CS (S (NP (ART Der) (NN Einfluss) (NP...\n",
       "2564    ['(ROOT (S (NN Studiengebühren) (VVFIN diskrim...\n",
       "Name: parsedText, Length: 2565, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"parsedText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
