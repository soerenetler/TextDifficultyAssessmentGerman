{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compound splitter\n",
    "'''\n",
    "\n",
    "# https://github.com/dtuggener/CharSplit\n",
    "import urllib\n",
    "url = \"https://github.com/dtuggener/CharSplit/archive/master.zip\"\n",
    "filename = \"CharSplit.zip\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "import os, sys\n",
    "#os.rename(\"CharSplit-master\", \"CharSplit\")\n",
    "from shutil import copyfile\n",
    "for filename in os.listdir(\"CharSplit-master\"):\n",
    "    if filename != \"README.md\": \n",
    "        copyfile(\"CharSplit-master/\" + filename, filename)\n",
    "    \n",
    "import char_split, ngram_probs\n",
    "def compoundSplit(string):\n",
    "    request = char_split.split_compound(string)[0]\n",
    "    if request[0] <= 0:\n",
    "        return [string]\n",
    "    else:\n",
    "        return compoundSplit(request[1]) + compoundSplit(request[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Functions for obtaining the morphological features.  Goals:\n",
    "\n",
    "• Derivational\n",
    "- hand-compiled list by Hancke et al (2012) \n",
    "\n",
    "• Inflectional\n",
    "- mood, person, tense, type of verbs\n",
    "- case of nouns\n",
    "\n",
    "• Compound words\n",
    "\n",
    "'''\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict, Counter, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import io\n",
    "import ast\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/soerenetler/TextDifficultyAssessmentGerman/master/preprocessed_text_df.csv\"\n",
    "df= pd.read_csv(url, sep=\"|\", lineterminator='\\n')\n",
    "\n",
    "# preprocessing Level classifications to get rid of unwanted data and \"B1 \" vs. \"B1\"\n",
    "df = df[(df[\"Level\"] == \"A1\")|(df[\"Level\"] == \"A2\")|(df[\"Level\"] == \"B1\")|(df[\"Level\"] == \"B1 \")|(df[\"Level\"] == \"B2\")|(df[\"Level\"] == \"C1\")|(df[\"Level\"] == \"C2\")]\n",
    "df = df.replace(\"B1 \",\"B1\")\n",
    "\n",
    "# returns a list of all the features included in the dataset, to be used as attributes in feature vector\n",
    "def get_all_features(df):\n",
    "    all_features = [] \n",
    "    index = 0\n",
    "    for all_sentences in df.loc[:,'RFTagger']:\n",
    "        if all_sentences is not None:\n",
    "            all_sentences = all_sentences[:-5] # remove the empty last sentence [] as following line won't work\n",
    "            all_sentences += \"]\" # nasty preprocessing necessary to keep list structure for literal_eval\n",
    "            all_sentences = ast.literal_eval(all_sentences)\n",
    "            for sentence in all_sentences:\n",
    "                for word_features in sentence:\n",
    "                    word_features = word_features[1:] #exclude the word itself and lemma at end (leads to too many features)                    \n",
    "                    for token in word_features:\n",
    "                        token = \"MO-\"+token\n",
    "                        if token not in all_features:\n",
    "                            all_features.append(token)\n",
    "            index += 1\n",
    "                    \n",
    "    return all_features\n",
    "\n",
    "def get_classifications(df):\n",
    "    y = []\n",
    "    \n",
    "    for classification in df.loc[:,'Level']:\n",
    "        y.append(classification)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def add_morph_columns(df_document):\n",
    "    df_document[\"MO-inf2verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-part2verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-imp2verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-1st2finverbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-2nd2finverbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-3rd2finverbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-subj2finverbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-finverbs2verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-modverbs2verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-auxverbs2verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-verbspersent\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-nom2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-gen2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-dat2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-acc2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-keit2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-ung2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-werk2nouns\"] = np.zeros(len(df_document))\n",
    "    #df_document[\"MO-derived2nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-compounds2nouns\"] = np.zeros(len(df_document))\n",
    "    \n",
    "def fill_morph_features(df_document, word_list, morph_dict, index):\n",
    "    derived_list = [\"ant\", \"anten\", \"antin\", \"antinnen\", \"arium\", \"arien\", \"ast\", \"asten\", \"astin\", \"astinnen\", \n",
    "                    \"at\", \"ate\", \"ator\", \"atoren\", \"atorin\", \"atorinnen\", \"atur\", \"aturen\", \"ei\", \"eien\", \"er\", \n",
    "                    \"erin\", \"erinnen\", \"ent\", \"ents\", \"enz\", \"enzen\", 'eur', 'eure', 'eurin', 'eurinnen', 'heit', \n",
    "                    'heiten', 'ist', 'isten', 'istin', 'istinnen', 'ion', 'ionen', 'ismus', 'ismen', 'ität', \n",
    "                    'itäten', 'keit', 'keiten', 'ling', 'lingen', 'nis', 'nisse', 'schaft', 'schaften', 'tum', \n",
    "                    'tümer', 'ung', 'ungen', 'ur', 'werk', 'werke', 'wesen']\n",
    "    \n",
    "    total_derived = 0\n",
    "    total_nouns = morph_dict[\"N\"]\n",
    "    total_verbs = morph_dict[\"VFIN\"] + morph_dict[\"VINF\"] + morph_dict[\"VIMP\"] + morph_dict[\"VPP\"]\n",
    "    total_keit = 0\n",
    "    total_ung = 0\n",
    "    total_werk = 0\n",
    "    total_nom = morph_dict[\"Nom\"]\n",
    "    total_gen = morph_dict[\"Gen\"]\n",
    "    total_dat = morph_dict[\"Dat\"]\n",
    "    total_acc = morph_dict[\"Acc\"]\n",
    "    total_subj = morph_dict[\"Subj\"]\n",
    "    total_first = morph_dict[\"1\"]\n",
    "    total_second = morph_dict[\"2\"]\n",
    "    total_third = morph_dict[\"3\"]\n",
    "    total_compounds = 0\n",
    "    total_infinitives = morph_dict[\"VINF\"]\n",
    "    total_participles = morph_dict[\"PART\"]\n",
    "    total_imperatives = morph_dict[\"VIMP\"]\n",
    "    total_finite = morph_dict[\"VFIN\"]\n",
    "    total_modal = morph_dict[\"MOD\"]\n",
    "    total_auxiliary = morph_dict[\"AUX\"]\n",
    "    num_sentences = 1 + word_list.count(\".\")\n",
    "    \n",
    "    for word in word_list:          \n",
    "        for derivation in derived_list:\n",
    "            if word[(len(word)-len(derivation)):len(word)] == derivation:\n",
    "                #total_derived += 1\n",
    "                    \n",
    "                if derivation == \"keit\":\n",
    "                    total_keit += 1\n",
    "                elif derivation == \"ung\":\n",
    "                    total_ung += 1\n",
    "                elif derivation == \"werk\":\n",
    "                    total_werk += 1\n",
    "                    \n",
    "        try: compoundSplit(word)\n",
    "        except:\n",
    "            total_compounds += 1\n",
    "            continue\n",
    "        if len(compoundSplit(word)) > 1:\n",
    "            total_compounds += 1\n",
    "                    \n",
    "    try: 1/total_nouns # catch divide by 0 error\n",
    "    except:\n",
    "        df_document.loc[index,\"MO-inf2verbs\"] = total_infinitives/total_verbs\n",
    "        df_document.loc[index,\"MO-part2verbs\"] = total_participles/total_verbs\n",
    "        df_document.loc[index,\"MO-imp2verbs\"] = total_imperatives/total_verbs\n",
    "        df_document.loc[index,\"MO-1st2finverbs\"] = total_first/total_finite\n",
    "        df_document.loc[index,\"MO-2nd2finverbs\"] = total_second/total_finite\n",
    "        df_document.loc[index,\"MO-3rd2finverbs\"] = total_third/total_finite\n",
    "        df_document.loc[index,\"MO-subj2finverbs\"] = total_subj/total_finite\n",
    "        df_document.loc[index,\"MO-finverbs2verbs\"] = total_finite/total_verbs\n",
    "        df_document.loc[index,\"MO-modverbs2verbs\"] = total_modal/total_verbs\n",
    "        df_document.loc[index,\"MO-auxverbs2verbs\"] = total_auxiliary/total_verbs\n",
    "        df_document.loc[index,\"MO-nom2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-gen2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-dat2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-acc2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-keit2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-ung2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-werk2nouns\"] = 0\n",
    "        #df_document.loc[index,\"MO-derived2nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-compounds2nouns\"] = 0\n",
    "        return\n",
    "        \n",
    "    try: 1/total_verbs # catch divide by 0 error\n",
    "    except:\n",
    "        df_document.loc[index,\"MO-inf2verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-part2verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-imp2verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-1st2finverbs\"] = 0\n",
    "        df_document.loc[index,\"MO-2nd2finverbs\"] = 0\n",
    "        df_document.loc[index,\"MO-3rd2finverbs\"] = 0\n",
    "        df_document.loc[index,\"MO-subj2finverbs\"] = 0\n",
    "        df_document.loc[index,\"MO-finverbs2verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-modverbs2verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-auxverbs2verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-nom2nouns\"] = total_nom/total_nouns\n",
    "        df_document.loc[index,\"MO-gen2nouns\"] = total_gen/total_nouns\n",
    "        df_document.loc[index,\"MO-dat2nouns\"] = total_dat/total_nouns\n",
    "        df_document.loc[index,\"MO-acc2nouns\"] = total_acc/total_nouns\n",
    "        df_document.loc[index,\"MO-keit2nouns\"] = total_keit/len(word_list)\n",
    "        df_document.loc[index,\"MO-ung2nouns\"] = total_ung/len(word_list)\n",
    "        df_document.loc[index,\"MO-werk2nouns\"] = total_werk/len(word_list)\n",
    "        #df_document.loc[index,\"MO-derived2nouns\"] = total_derived/total_nouns\n",
    "        df_document.loc[index,\"MO-compounds2nouns\"] = total_compounds/total_nouns\n",
    "        return\n",
    "              \n",
    "    df_document.loc[index,\"MO-inf2verbs\"] = total_infinitives/total_verbs\n",
    "    df_document.loc[index,\"MO-part2verbs\"] = total_participles/total_verbs\n",
    "    df_document.loc[index,\"MO-imp2verbs\"] = total_imperatives/total_verbs\n",
    "    df_document.loc[index,\"MO-1st2finverbs\"] = total_first/total_finite\n",
    "    df_document.loc[index,\"MO-2nd2finverbs\"] = total_second/total_finite\n",
    "    df_document.loc[index,\"MO-3rd2finverbs\"] = total_third/total_finite\n",
    "    df_document.loc[index,\"MO-subj2finverbs\"] = total_subj/total_finite\n",
    "    df_document.loc[index,\"MO-finverbs2verbs\"] = total_finite/total_verbs\n",
    "    df_document.loc[index,\"MO-modverbs2verbs\"] = total_modal/total_verbs\n",
    "    df_document.loc[index,\"MO-auxverbs2verbs\"] = total_auxiliary/total_verbs\n",
    "    df_document.loc[index,\"MO-verbspersent\"] = total_verbs/num_sentences\n",
    "    df_document.loc[index,\"MO-nom2nouns\"] = total_nom/total_nouns\n",
    "    df_document.loc[index,\"MO-gen2nouns\"] = total_gen/total_nouns\n",
    "    df_document.loc[index,\"MO-dat2nouns\"] = total_dat/total_nouns\n",
    "    df_document.loc[index,\"MO-acc2nouns\"] = total_acc/total_nouns\n",
    "    df_document.loc[index,\"MO-keit2nouns\"] = total_keit/len(word_list)\n",
    "    df_document.loc[index,\"MO-ung2nouns\"] = total_ung/len(word_list)\n",
    "    df_document.loc[index,\"MO-werk2nouns\"] = total_werk/len(word_list)\n",
    "    #df_document.loc[index,\"MO-derived2nouns\"] = total_derived/total_nouns\n",
    "    df_document.loc[index,\"MO-compounds2nouns\"] = total_compounds/total_nouns\n",
    "    \n",
    "def fill_document_vector(df,df_document):\n",
    "    add_morph_columns(df_document)\n",
    "    index = 0\n",
    "    for all_sentences in df.loc[:,'RFTagger']:\n",
    "        if all_sentences is not None:\n",
    "            all_sentences = all_sentences[:-5] # remove the empty last sentence [] as following line won't work\n",
    "            all_sentences += \"]\" # nasty preprocessing necessary to keep list structure for literal_eval\n",
    "            all_sentences = ast.literal_eval(all_sentences)\n",
    "            text_length = 0\n",
    "            word_list = []\n",
    "            morph_dict = defaultdict(int)\n",
    "            for sentence in all_sentences:\n",
    "                text_length += len(sentence)\n",
    "                for word_features in sentence:\n",
    "                    word_list.append(word_features[0])\n",
    "                    word_features = word_features[1:] #exclude the word itself and lemma at end (leads to too many features)\n",
    "                    for token in word_features:\n",
    "                        df_document.loc[index,\"MO-\"+token] +=1\n",
    "                        if token == \"Nom\" or token== \"Gen\" or token==\"Dat\" or token==\"Acc\":\n",
    "                            if \"N\" in word_features: # only add nouns with above cases\n",
    "                                morph_dict[token] += 1\n",
    "                                continue\n",
    "                            else: continue\n",
    "                        morph_dict[token] += 1\n",
    "            df_document.loc[index] = df_document.loc[index] / text_length # normalize each value with the total text length       \n",
    "            fill_morph_features(df_document, word_list, morph_dict, index)\n",
    "            index +=1\n",
    "            \n",
    "def save_df(path,df):\n",
    "    df.to_csv(path,sep=\"|\",index=False)\n",
    "\n",
    "# get total number of sentences and all features contained in dataset in order to create vector columns\n",
    "all_features = get_all_features(df)  \n",
    "\n",
    "# document level vector, i.e. each row in the feature vector represents one document\n",
    "zero_data = np.zeros(shape=(len(df),len(all_features))) # create 1 row in the feature vector per document\n",
    "df_document = pd.DataFrame(zero_data,columns=sorted(all_features))\n",
    "y = get_classifications(df)\n",
    "\n",
    "fill_document_vector(df,df_document)\n",
    "display(df_document.head())\n",
    "print(list(df_document))\n",
    "\n",
    "filename = \"06_MorphologicalFeatures_df.csv\"\n",
    "save_df(filename,df_document)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
