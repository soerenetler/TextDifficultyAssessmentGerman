{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compound splitter\n",
    "'''\n",
    "\n",
    "# https://github.com/dtuggener/CharSplit\n",
    "import urllib\n",
    "url = \"https://github.com/dtuggener/CharSplit/archive/master.zip\"\n",
    "filename = \"CharSplit.zip\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "import os, sys\n",
    "#os.rename(\"CharSplit-master\", \"CharSplit\")\n",
    "from shutil import copyfile\n",
    "for filename in os.listdir(\"CharSplit-master\"):\n",
    "    if filename != \"README.md\": \n",
    "        copyfile(\"CharSplit-master/\" + filename, filename)\n",
    "    \n",
    "import char_split, ngram_probs\n",
    "def compoundSplit(string):\n",
    "    request = char_split.split_compound(string)[0]\n",
    "    if request[0] <= 0:\n",
    "        return [string]\n",
    "    else:\n",
    "        return compoundSplit(request[1]) + compoundSplit(request[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "['MO-APPR', 'MO-Dat', 'MO-PRO', 'MO-Poss', 'MO-Attr', 'MO--', 'MO-Sg', 'MO-Fem', 'MO-N', 'MO-Reg', 'MO-VFIN', 'MO-Full', 'MO-3', 'MO-Pl', 'MO-Pres', 'MO-Ind', 'MO-CARD', 'MO-Nom', 'MO-SYM', 'MO-Pun', 'MO-Sent', 'MO-ART', 'MO-Def', 'MO-Sein', 'MO-1', 'MO-Pers', 'MO-Subst', 'MO-*', 'MO-CONJ', 'MO-Coord', 'MO-ADV', 'MO-ADJD', 'MO-Pos', 'MO-Masc', 'MO-PART', 'MO-Verb', 'MO-Haben', 'MO-Acc', 'MO-Neut', 'MO-Comma', 'MO-Indef', 'MO-Rel', 'MO-ADJA', 'MO-In', 'MO-Mod', 'MO-APPRART', 'MO-Neg', 'MO-VINF', 'MO-Name', 'MO-Außer', 'MO-Comp', 'MO-VPP', 'MO-Psp', 'MO-Aber', 'MO-Gen', 'MO-Auf', 'MO-ITJ', 'MO-Past', 'MO-Subj', 'MO-Aux', 'MO-Denn', 'MO-An', 'MO-Sup', 'MO-Refl', 'MO-PROADV', 'MO-Dem', 'MO-Deg', 'MO-Colon', 'MO-Quot', 'MO-Left', 'MO-Ans', 'MO-2', 'MO-Inter', 'MO-Right', 'MO-SubFin', 'MO-Vor', 'MO-Wie', 'MO-Other', 'MO-XY', 'MO-Als', 'MO-Unter', 'MO-Zu', 'MO-SubInf', 'MO-VIMP', 'MO-Über', 'MO-Per', 'MO-Bis', 'MO-zu', 'MO-FM', 'MO-Slash', 'MO-Paren', 'MO-Hinter', 'MO-TRUNC', 'MO-Noun', 'MO-Zwischen', 'MO-Pro', 'MO-APZR', 'MO-Hyph', 'MO-Doch', 'MO-Cont', 'MO-APPO', 'MO-Adj', 'MO-Noch']\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-87215230075b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classifications\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m \u001b[0mfill_document_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-87215230075b>\u001b[0m in \u001b[0;36mfill_document_vector\u001b[0;34m(df, df_document)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         \u001b[0mdf_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"MO-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mdf_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtext_length\u001b[0m \u001b[0;31m# normalize each value with the total text length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mfill_morph_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_document\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-87215230075b>\u001b[0m in \u001b[0;36mfill_morph_features\u001b[0;34m(df_document, all_sentences, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompoundSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mderivation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mderived_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-eacd41911881>\u001b[0m in \u001b[0;36mcompoundSplit\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompoundSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcompoundSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-4-eacd41911881>\u001b[0m in \u001b[0;36mcompoundSplit\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompoundSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcompoundSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Functions for obtaining the morphological features.  Goals:\n",
    "\n",
    "• Derivational\n",
    "- hand-compiled list by Hancke et al (2012) \n",
    "\n",
    "• Inflectional\n",
    "- mood, person, tense, type of verbs\n",
    "- case of nouns\n",
    "\n",
    "• Compound words\n",
    "\n",
    "word embeddings?\n",
    "\n",
    "save features and output to df\n",
    "\n",
    "'''\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict, Counter, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import io\n",
    "import ast\n",
    "\n",
    "df= pd.read_csv(\"05_FrequencyFeatures_df.csv\", sep=\"|\", lineterminator='\\n')\n",
    "\n",
    "# preprocessing Level classifications to get rid of unwanted data and \"B1 \" vs. \"B1\"\n",
    "df = df[(df[\"Level\"] == \"A1\")|(df[\"Level\"] == \"A2\")|(df[\"Level\"] == \"B1\")|(df[\"Level\"] == \"B1 \")|(df[\"Level\"] == \"B2\")|(df[\"Level\"] == \"C1\")|(df[\"Level\"] == \"C2\")]\n",
    "df = df.replace(\"B1 \",\"B1\")\n",
    "\n",
    "# returns a list of all the features included in the dataset, to be used as attributes in feature vector\n",
    "def get_all_features(df):\n",
    "    all_features = [] \n",
    "    sentence_count = 0\n",
    "    index = 0\n",
    "    for all_sentences in df.loc[:,'RFTagger']:\n",
    "        if all_sentences is not None:\n",
    "            all_sentences = all_sentences[:-5] # remove the empty last sentence [] as following line won't work\n",
    "            all_sentences += \"]\" # nasty preprocessing necessary to keep list structure for literal_eval\n",
    "            all_sentences = ast.literal_eval(all_sentences)\n",
    "            for sentence in all_sentences:\n",
    "                sentence_count +=1\n",
    "                for word_features in sentence:\n",
    "                    word_features = word_features[1:] #exclude the word itself and lemma at end (leads to too many features)                    \n",
    "                    for token in word_features:\n",
    "                        token = \"MO-\"+token\n",
    "                        if token not in all_features:\n",
    "                            all_features.append(token)\n",
    "            index += 1\n",
    "                    \n",
    "    return sentence_count, all_features\n",
    "\n",
    "def get_classifications(df):\n",
    "    y = []\n",
    "    \n",
    "    for classification in df.loc[:,'Level']:\n",
    "        y.append(classification)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def get_classifications_sentences(df):\n",
    "    return\n",
    "\n",
    "def fill_sentence_vector(df,df_sentence):\n",
    "    index = 0\n",
    "    for all_sentences in df.loc[:,'RFTagger']:\n",
    "        if all_sentences is not None:\n",
    "            all_sentences = all_sentences[:-5] # remove the empty last sentence [] as following line won't work\n",
    "            all_sentences += \"]\" # nasty preprocessing necessary to keep list structure for literal_eval\n",
    "            all_sentences = ast.literal_eval(all_sentences)\n",
    "            for sentence in all_sentences:\n",
    "                for word_features in sentence:\n",
    "                    word_features = word_features[1:] #exclude the word itself and lemma at end (leads to too many features)\n",
    "                    for token in word_features:\n",
    "                        df_sentence.loc[index,token] +=1\n",
    "                index +=1\n",
    "\n",
    "def add_morph_columns(df_document):\n",
    "    df_document[\"MO-keit\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-ung\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-werk\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-derived to nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-nom. to nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-gen. to nouns\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-subj. to verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-2p to verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-3p to verbs\"] = np.zeros(len(df_document))\n",
    "    df_document[\"MO-compounds to nouns\"] = np.zeros(len(df_document))\n",
    "    \n",
    "def fill_morph_features(df_document, all_sentences, index):\n",
    "    derived_list = [\"ant\", \"anten\", \"antin\", \"antinnen\", \"arium\", \"arien\", \"ast\", \"asten\", \"astin\", \"astinnen\", \n",
    "                    \"at\", \"ate\", \"ator\", \"atoren\", \"atorin\", \"atorinnen\", \"atur\", \"aturen\", \"ei\", \"eien\", \"er\", \n",
    "                    \"erin\", \"erinnen\", \"ent\", \"ents\", \"enz\", \"enzen\", 'eur', 'eure', 'eurin', 'eurinnen', 'heit', \n",
    "                    'heiten', 'ist', 'isten', 'istin', 'istinnen', 'ion', 'ionen', 'ismus', 'ismen', 'ität', \n",
    "                    'itäten', 'keit', 'keiten', 'ling', 'lingen', 'nis', 'nisse', 'schaft', 'schaften', 'tum', \n",
    "                    'tümer', 'ung', 'ungen', 'ur', 'werk', 'werke', 'wesen']\n",
    "    total_derived = 0\n",
    "    total_nouns = 0\n",
    "    total_verbs = 0\n",
    "    total_keit = 0\n",
    "    total_ung = 0\n",
    "    total_werk = 0\n",
    "    total_nom = 0\n",
    "    total_gen = 0\n",
    "    total_subj = 0\n",
    "    total_second = 0\n",
    "    total_third = 0\n",
    "    total_compounds = 0\n",
    "    \n",
    "    for sentence in all_sentences:\n",
    "        for word_features in sentence:\n",
    "            word = word_features[0]\n",
    "            \n",
    "            print(len(compoundSplit(word)))\n",
    "            \n",
    "            for derivation in derived_list:\n",
    "                if word[(len(word)-len(derivation)):len(word)] == derivation:\n",
    "                    total_derived += 1\n",
    "                    \n",
    "                    if derivation == \"keit\":\n",
    "                        total_keit += 1\n",
    "                    elif derivation == \"ung\":\n",
    "                        total_ung += 1\n",
    "                    elif derivation == \"werk\":\n",
    "                        total_werk += 1\n",
    "                    \n",
    "            if \"N\" in word_features or \"Noun\" in word_features:\n",
    "                total_nouns += 1\n",
    "            if \"VFIN\" in word_features or \"VINF\" in word_features or \"VIMP\" in word_features or \"VPP\" in word_features or \"Verb\" in word_features:\n",
    "                total_verbs += 1\n",
    "            if \"Nom\" in word_features:\n",
    "                total_nom += 1\n",
    "            if \"Gen\" in word_features:\n",
    "                total_gen += 1\n",
    "            if \"Subj\" in word_features:\n",
    "                total_subj += 1\n",
    "            if \"2\" in word_features:\n",
    "                total_second += 1\n",
    "            if \"3\" in word_features:\n",
    "                total_third += 1\n",
    "                \n",
    "    if total_nouns == 0:\n",
    "        df_document.loc[index,\"MO-keit\"] = 0\n",
    "        df_document.loc[index,\"MO-ung\"] = 0\n",
    "        df_document.loc[index,\"MO-werk\"] = 0\n",
    "        df_document.loc[index,\"MO-derived to nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-nom. to nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-gen. to nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-compounds to nouns\"] = 0\n",
    "        df_document.loc[index,\"MO-subj. to verbs\"] = total_subj/total_verbs\n",
    "        df_document.loc[index,\"MO-2p to verbs\"] = total_second/total_verbs\n",
    "        df_document.loc[index,\"MO-3p to verbs\"] = total_third/total_verbs\n",
    "        return\n",
    "    elif total_verbs == 0: \n",
    "        df_document.loc[index,\"MO-subj. to verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-2p to verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-3p to verbs\"] = 0\n",
    "        df_document.loc[index,\"MO-keit\"] = total_keit/total_nouns\n",
    "        df_document.loc[index,\"MO-ung\"] = total_ung/total_nouns\n",
    "        df_document.loc[index,\"MO-werk\"] = total_werk/total_nouns\n",
    "        df_document.loc[index,\"MO-derived to nouns\"] = total_derived/total_nouns\n",
    "        df_document.loc[index,\"MO-nom. to nouns\"] = total_nom/total_nouns\n",
    "        df_document.loc[index,\"MO-gen. to nouns\"] = total_gen/total_nouns\n",
    "        df_document.loc[index,\"MO-compounds to nouns\"] = total_compounds/total_nouns\n",
    "        return\n",
    "    \n",
    "    df_document.loc[index,\"MO-keit\"] = total_keit/total_nouns\n",
    "    df_document.loc[index,\"MO-ung\"] = total_ung/total_nouns\n",
    "    df_document.loc[index,\"MO-werk\"] = total_werk/total_nouns\n",
    "    df_document.loc[index,\"MO-derived to nouns\"] = total_derived/total_nouns\n",
    "    df_document.loc[index,\"MO-nom. to nouns\"] = total_nom/total_nouns\n",
    "    df_document.loc[index,\"MO-gen. to nouns\"] = total_gen/total_nouns\n",
    "    df_document.loc[index,\"MO-subj. to verbs\"] = total_subj/total_verbs\n",
    "    df_document.loc[index,\"MO-2p to verbs\"] = total_second/total_verbs\n",
    "    df_document.loc[index,\"MO-3p to verbs\"] = total_third/total_verbs\n",
    "    df_document.loc[index,\"MO-compounds to nouns\"] = total_compounds/total_nouns\n",
    "\n",
    "def fill_document_vector(df,df_document):\n",
    "    add_morph_columns(df_document)\n",
    "    index = 0\n",
    "    for all_sentences in df.loc[:,'RFTagger']:\n",
    "        if all_sentences is not None:\n",
    "            all_sentences = all_sentences[:-5] # remove the empty last sentence [] as following line won't work\n",
    "            all_sentences += \"]\" # nasty preprocessing necessary to keep list structure for literal_eval\n",
    "            all_sentences = ast.literal_eval(all_sentences)\n",
    "            text_length = 0\n",
    "            for sentence in all_sentences:\n",
    "                text_length += len(sentence)\n",
    "                for word_features in sentence:\n",
    "                    word_features = word_features[1:] #exclude the word itself and lemma at end (leads to too many features)\n",
    "                    for token in word_features:\n",
    "                        df_document.loc[index,\"MO-\"+token] +=1\n",
    "            df_document.loc[index] = df_document.loc[index] / text_length # normalize each value with the total text length       \n",
    "            fill_morph_features(df_document, all_sentences, index)\n",
    "            index +=1\n",
    "\n",
    "# get total number of sentences and all features contained in dataset in order to create vector columns\n",
    "num_sentences, all_features = get_all_features(df)  \n",
    "print(len(all_features))\n",
    "print(all_features)\n",
    "\n",
    "'''\n",
    "# sentence level vector, i.e. each row in the feature vector represents one sentence\n",
    "zero_data = np.zeros(shape=(num_sentences,len(all_features)))\n",
    "df_sentence = pd.DataFrame(zero_data, columns=sorted(all_features))\n",
    "y = get_classifications(df)\n",
    "\n",
    "fill_sentence_vector(df,df_sentence)\n",
    "print(df_sentence.shape)\n",
    "'''\n",
    "\n",
    "# document level vector, i.e. each row in the feature vector represents one document\n",
    "zero_data = np.zeros(shape=(len(df),len(all_features))) # create 1 row in the feature vector per document\n",
    "df_document = pd.DataFrame(zero_data,columns=sorted(all_features))\n",
    "y = get_classifications(df)\n",
    "\n",
    "fill_document_vector(df,df_document)\n",
    "display(df_document.head())\n",
    "\n",
    "def save_df(path,df):\n",
    "    df.to_csv(path,sep=\"|\",index=False)\n",
    "\n",
    "filename = \"06_MorphologicalFeatures_df.csv\"\n",
    "save_df(filename,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow implementation\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# get a list of all words in the dataframe for word2vec processing            \n",
    "def vocabulary_by_level(df):\n",
    "    levels = [\"A1\",\"A2\",\"B1\",\"B2\",\"C1\",\"C2\"]\n",
    "    levels_dict = defaultdict(list)\n",
    "    \n",
    "    for level in levels:\n",
    "        for text in df[df[\"Level\"] == level].loc[:,\"Text\"]:\n",
    "            text = word_tokenize(text)\n",
    "            for token in text:\n",
    "                levels_dict[level].append(token)\n",
    "    return levels_dict\n",
    "\n",
    "# uses build_dataset to extract n most common words and assign unique integer to each word.  Returns\n",
    "# dicts which can look up integer-word and word-integer pairs.\n",
    "def get_embeddings_dicts(levels_dict, vocabulary_size):\n",
    "    embeddings_dict = dict()\n",
    "    \n",
    "    for key in levels_dict.keys():\n",
    "        data, count, dictionary, reversed_dictionary = build_dataset(levels_dict[key],vocabulary_size)\n",
    "        embeddings_dict[key] = [data,count,dictionary,reversed_dictionary]\n",
    "    \n",
    "    return embeddings_dict\n",
    "\n",
    "# Extract the n most common words from each level's texts.  \n",
    "# Credit to http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n",
    "\n",
    "vocabulary_size = 428\n",
    "levels_dict = vocabulary_by_level(df)\n",
    "embeddings_dict = get_embeddings_dicts(levels_dict, vocabulary_size\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a context\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def run(graph, num_steps, class_tuple):\n",
    "    data = class_tuple[0]\n",
    "    count = class_tuple[1]\n",
    "    dictionary = class_tuple[2]\n",
    "    reverse_dictionary = class_tuple[3]\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(data,\n",
    "                batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "    \n",
    "                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "        \n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "            \n",
    "with graph.as_default():\n",
    "\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # Construct the variables for the softmax\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "\n",
    "    # convert train_context to a one-hot format\n",
    "    train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "    \n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "num_steps = 50000\n",
    "\n",
    "for class_level in embeddings_dict:\n",
    "    print(\"Starting!  class level is \",class_level)\n",
    "    nce_start_time = dt.datetime.now()\n",
    "    class_tuple = embeddings_dict[class_level]\n",
    "    run(graph,num_steps,class_tuple)\n",
    "    nce_end_time = dt.datetime.now()\n",
    "    print(\"NCE method took {} seconds to run 50000 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_document, y, test_size=0.2)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "level_pred = clf.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, level_pred)\n",
    "class_names = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
